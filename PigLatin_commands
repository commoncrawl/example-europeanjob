
1) Example how to use PigLatin commands in interactive mode to d further work on the ouput from MapReduce job. One of our MapReduce jobs finds the candidate vacancy URLs. The output from that job was <(domainid,URL),(matched_key,frequency)> after running these commands we get <domainid, URL>

//load the file which contains the the candidate URLs with the matched keywords
//two column with tab separated; key: (domainID,URL) "\t" (value:matchedKeywrds,frequency)
> candidate_urls = load ‘input file’ as (key:chararray, val:chararray);
//group input using key:domainID,URL, this will result unique key 
> group_candidate_urls = group candidate_urls by key ;
//generate only the key ==> we need for sampling the domainIDs and the URLs
> generate_candidate_url_key = foreach group_candidate_urls generate key;
//the domainID and the URL are comma separated so we use STRSPLIT to split them, and use flatten to put them in separate colomns
>flatten_candidate_url_key = foreach generate_candidate_url_key generate flatten(STRSPLIT ($0, ‘,’ , 2)) as (domainid:chararray, url:chararray);
//order the result by the domainid
>order_candidate_url_by_doaminid = order flatten_candidate_url_key by domainid ; 

//the order_candidate_url_by_doaminid relation contains candidate URLs and their domain ids: domainID \t URL, we use store command to execute all the above commands and stor result in HDFS
>store order_candidate_url_by_doaminid into 'out path'


2) We can use the output from point (1) to count number of candidate vacancies for each domain ID

> candidate_urls = load 'input file' as (domainid:chararray,url:chararray);
//group by domain id and count the group urls
> group_urls_bydomainid = group candidate_urls by $0 ;
> result_domainid_count = foreach group_urls_bydomainid {
unique_urls = DISTINCT $1;
generate group, COUNT(unique_urls);
                };




3) Join two results from two MapReduce jobs

join urls sample list with common crawl lookup index
URLsSampleList contains the list of urls generated by sampler, the structure of file: (domainID "\t" URL)
arcLookupIX is the file which contain information about where to find each URL; in which arc file. file structure: (domainID "\t" URL "\t" metadataFile "\t" arcFile "\t" offset)
The infrmation has been extracted from the meta data files
arcFile: for example 1346823846036/1346848925218_214.arc.gz. to find the file we need to specify the complete path, which is for this file
/data/public/common-crawl/parse-output/segment/1346823846036/1346848925218_214.arc.gz


We join the two files by url to get which ARC files contains the URLs. we uses special type of join called 'replicated' join, it works when one of the relations is small enough to fit in memory. The big relatin(index) with small one (sample list), where the later should be small enough to fit in memory

> sample = load 'samplefile' as (domainid, url);
>index = load 'indexFile' as (domainid, url, metafile, arcfile, offset);
//replicated join, name of big relation should be specified first
>sampleindex = join index by url, sample by url using 'replicated';
> arcfiles = foreach urlarc generate CONCAT('/data/public/common-crawl/parse-output/segment/',arc) ;




